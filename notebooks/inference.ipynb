{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9e44b3675389ea",
   "metadata": {},
   "source": [
    "# Inference with METL models\n",
    "This notebook shows how to run inference with METL models trained in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdeaafc4c7596288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2949c7b42a4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# define the name of the project root directory\n",
    "project_root_dir_name = \"metl\"\n",
    "\n",
    "# find the project root by checking each parent directory\n",
    "current_dir = os.getcwd()\n",
    "while os.path.basename(current_dir) != project_root_dir_name and current_dir != os.path.dirname(current_dir):\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# change the current working directory to the project root directory\n",
    "if os.path.basename(current_dir) == project_root_dir_name:\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    print(\"project root directory not found\")\n",
    "    \n",
    "# add the project code folder to the system path so imports work\n",
    "module_path = os.path.abspath(\"code\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64354ce02b142dcd",
   "metadata": {},
   "source": [
    "# Using our inference framework\n",
    "\n",
    "We provide the script [inference.py](../code/inference.py) for running inference with models trained in this repository. It supports similar arguments and datamodule capabilities as used for training the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ba01f-03da-499c-9126-a3ddd7f2692f",
   "metadata": {},
   "source": [
    "The arguements `--write_interval` and `--batch_write_mode` control how often predictions are saved and in what format. \n",
    "\n",
    "The `write_interval` can be set to \"batch\", \"epoch\", or \"batch_and_epoch\". When set to \"batch\", predictions will be saved to disk after each batch. When set to \"epoch\", predictions will first be stored in RAM until all data has been processed, and then they will be written to disk. If you have a lot of data which might not fit in RAM, then you will want to set `--write_interval` to \"batch\" (default).\n",
    "\n",
    "The `--batch_write_mode` can be set to \"combined_csv\", \"separate_csv\", or \"separate_npy\". When set to \"combined_csv\", there will be a single output csv file, and it will be appended to after each batch is processed. When set to either \"separate_csv\" or \"separate_npy\", there will be a separate output file for each batch in either .csv or .npy format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014acc1-af5e-4c3b-9a0d-d0a69976c772",
   "metadata": {},
   "source": [
    "## Source model example\n",
    "This repository contains a sample GFP Rosetta dataset and a pretrained METL-Local GFP source model, which we can use as examples. \n",
    "\n",
    "We specify the following arguments:\n",
    "\n",
    "| Argument               | Description                                                | Value                                      |\n",
    "|:------------------------|:------------------------------------------------------------|:--------------------------------------------|\n",
    "| `pretrained_ckpt_path` | Path to the pretrained model checkpoint                    | `pretrained_models/Hr4GNHws.pt`            |\n",
    "| `dataset_type`         | Type of dataset being used (rosetta or dms)                                | `rosetta`                                  |\n",
    "| `ds_fn`                | Path to the database file for the dataset                  | `data/rosetta_data/avgfp/avgfp.db`         |\n",
    "| `batch_size`           | Batch size used during inference               | `512`                                     |\n",
    "\n",
    "The inference script will automatically save output in the `output/inference` directory. There will be an output csv file for each processed batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5160d8ea88e32c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sg/PycharmProjects/metl/code/inference.py:159: UserWarning: Transforming checkpoint keys: strip_prefix='', add_prefix='model.'\n",
      "  warnings.warn(\n",
      "Using example_input_array with pdb_fn='1gfl_cm.pdb' and aa_seq_len=237\n",
      "Output directory: output/inference/Hr4GNHws/rosetta_avgfp/full_dataset\n",
      "Writing predictions to output/inference/Hr4GNHws/rosetta_avgfp/full_dataset/predictions.npy\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████████████| 20/20 [00:08<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "!python code/inference.py --pretrained_ckpt_path=pretrained_models/Hr4GNHws.pt --dataset_type=rosetta --ds_fn=data/rosetta_data/avgfp/avgfp.db --batch_size=512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415b48e-8c9c-4d86-b5c3-362374dbc875",
   "metadata": {},
   "source": [
    "By default, the script will compute predictions for the full dataset. If you only need to save predictions for a particular train, validation, or test set, you can do so by setting the `--split_dir` and `--predict_mode` arguments. The function call below will compute predictions just for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf7d281-4293-46df-b6e9-b49ebcf5b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sg/PycharmProjects/metl/code/inference.py:159: UserWarning: Transforming checkpoint keys: strip_prefix='', add_prefix='model.'\n",
      "  warnings.warn(\n",
      "Using example_input_array with pdb_fn='1gfl_cm.pdb' and aa_seq_len=237\n",
      "Output directory: output/inference/Hr4GNHws/rosetta_avgfp/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991/test\n",
      "Writing predictions to output/inference/Hr4GNHws/rosetta_avgfp/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991/test/predictions.npy\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|████████████████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "!python code/inference.py --pretrained_ckpt_path=pretrained_models/Hr4GNHws.pt --dataset_type=rosetta --ds_fn=data/rosetta_data/avgfp/avgfp.db --batch_size=512 --split_dir=data/rosetta_data/avgfp/splits/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991 --predict_mode=test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610a8a3-0353-43c2-935a-9aa53f9fdff7",
   "metadata": {},
   "source": [
    "## Target (finetuned) model example\n",
    "We first need to finetune a model using experimental data. Run the command below, which will finetune the pretrained model above using the GFP experimental dataset. Note we manually specify the UUID `examplemodel` for this model. See the [finetuning.ipynb](finetuning.ipynb) notebook for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6ee595-c810-404f-b639-6a513d98d461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed not specified, using: 855922268\n",
      "Global seed set to 855922268\n",
      "User gave model UUID: examplemodel\n",
      "Did not find existing log directory corresponding to given UUID: examplemodel\n",
      "Created log directory: output/training_logs/examplemodel\n",
      "Final UUID: examplemodel\n",
      "Final log directory: output/training_logs/examplemodel\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/metl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Number of training steps is 50\n",
      "Number of warmup steps is 0.5\n",
      "Second warmup phase starts at step 25\n",
      "total_steps 50\n",
      "phase1_total_steps 25\n",
      "phase2_total_steps 25\n",
      "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m  In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m Out sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model                  │ TransferMod… │  2.4 M │\u001b[37m \u001b[0m\u001b[37m[128, 237]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [128, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ model.model            │ SequentialW… │  2.4 M │\u001b[37m \u001b[0m\u001b[37m[128, 237]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [128, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ model.model.backbone   │ SequentialW… │  2.4 M │\u001b[37m \u001b[0m\u001b[37m[128, 237]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model.model.dropout    │ Dropout      │      0 │\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ model.model.flatten    │ Flatten      │      0 │\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ model.model.prediction │ Linear       │    257 │\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [128, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ test_pearson           │ PearsonCorr… │      0 │\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ test_spearman          │ SpearmanCor… │      0 │\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\n",
      "└───┴────────────────────────┴──────────────┴────────┴────────────┴────────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 257                                                           \n",
      "\u001b[1mNon-trainable params\u001b[0m: 2.4 M                                                     \n",
      "\u001b[1mTotal params\u001b[0m: 2.4 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 9                                       \n",
      "Starting sanity check...\n",
      "Sanity check complete.\n",
      "Starting training...\n",
      "Epoch     0: Train Loss =   2.249, Val Loss =   2.007\n",
      "Epoch     1: Train Loss =   2.304, Val Loss =   1.962\n",
      "Epoch     2: Train Loss =   2.182, Val Loss =   1.919\n",
      "Epoch     3: Train Loss =   2.111, Val Loss =   1.877\n",
      "Epoch     4: Train Loss =   2.064, Val Loss =   1.838\n",
      "Epoch     5: Train Loss =   2.020, Val Loss =   1.800\n",
      "Epoch     6: Train Loss =   2.012, Val Loss =   1.765\n",
      "Epoch     7: Train Loss =   1.999, Val Loss =   1.732\n",
      "Epoch     8: Train Loss =   1.940, Val Loss =   1.702\n",
      "Epoch     9: Train Loss =   1.886, Val Loss =   1.675\n",
      "Epoch    10: Train Loss =   1.889, Val Loss =   1.650\n",
      "Epoch    11: Train Loss =   1.838, Val Loss =   1.627\n",
      "Epoch    12: Train Loss =   1.775, Val Loss =   1.608\n",
      "Epoch    13: Train Loss =   1.820, Val Loss =   1.591\n",
      "Epoch    14: Train Loss =   1.766, Val Loss =   1.576\n",
      "Epoch    15: Train Loss =   1.759, Val Loss =   1.564\n",
      "Epoch    16: Train Loss =   1.720, Val Loss =   1.553\n",
      "Epoch    17: Train Loss =   1.754, Val Loss =   1.545\n",
      "Epoch    18: Train Loss =   1.738, Val Loss =   1.539\n",
      "Epoch    19: Train Loss =   1.717, Val Loss =   1.534\n",
      "Epoch    20: Train Loss =   1.758, Val Loss =   1.531\n",
      "Epoch    21: Train Loss =   1.682, Val Loss =   1.528\n",
      "Epoch    22: Train Loss =   1.747, Val Loss =   1.527\n",
      "Epoch    23: Train Loss =   1.695, Val Loss =   1.527\n",
      "Epoch    24: Train Loss =   1.696, Val Loss =   1.527\n",
      "Epoch    25: Train Loss =   1.683, Val Loss =   1.527\n",
      "Epoch    26: Train Loss =   1.740, Val Loss =   1.503\n",
      "Epoch    27: Train Loss =   1.632, Val Loss =   1.479\n",
      "Epoch    28: Train Loss =   1.644, Val Loss =   1.455\n",
      "Epoch    29: Train Loss =   1.587, Val Loss =   1.431\n",
      "Epoch    30: Train Loss =   1.585, Val Loss =   1.408\n",
      "Epoch    31: Train Loss =   1.553, Val Loss =   1.385\n",
      "Epoch    32: Train Loss =   1.531, Val Loss =   1.363\n",
      "Epoch    33: Train Loss =   1.541, Val Loss =   1.342\n",
      "Epoch    34: Train Loss =   1.445, Val Loss =   1.323\n",
      "Epoch    35: Train Loss =   1.404, Val Loss =   1.304\n",
      "Epoch    36: Train Loss =   1.418, Val Loss =   1.286\n",
      "Epoch    37: Train Loss =   1.345, Val Loss =   1.269\n",
      "Epoch    38: Train Loss =   1.322, Val Loss =   1.254\n",
      "Epoch    39: Train Loss =   1.349, Val Loss =   1.241\n",
      "Epoch    40: Train Loss =   1.306, Val Loss =   1.229\n",
      "Epoch    41: Train Loss =   1.269, Val Loss =   1.219\n",
      "Epoch    42: Train Loss =   1.238, Val Loss =   1.210\n",
      "Epoch    43: Train Loss =   1.254, Val Loss =   1.203\n",
      "Epoch    44: Train Loss =   1.275, Val Loss =   1.198\n",
      "Epoch    45: Train Loss =   1.220, Val Loss =   1.194\n",
      "Epoch    46: Train Loss =   1.246, Val Loss =   1.192\n",
      "Epoch    47: Train Loss =   1.243, Val Loss =   1.190\n",
      "Epoch    48: Train Loss =   1.167, Val Loss =   1.189\n",
      "Epoch    49: Train Loss =   1.236, Val Loss =   1.189\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "Restoring states from the checkpoint path at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Loaded model weights from checkpoint at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Starting testing...\n",
      "Testing complete.\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2121491432189941    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_pearson       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6022544503211975    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_spearman      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5882202982902527    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "Restoring states from the checkpoint path at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Loaded model weights from checkpoint at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Starting prediction...\n",
      "Prediction complete.\n",
      "saving a scatter plot for set: train (128 variants)\n",
      "saving a scatter plot for set: val (32 variants)\n",
      "saving a scatter plot for set: test (4655 variants)\n",
      "            mse  pearsonr        r2  spearmanr\n",
      "set                                           \n",
      "train  1.275039  0.723759 -0.092885   0.740277\n",
      "val    1.189313  0.561990 -0.102850   0.590963\n",
      "test   1.212149  0.602254 -0.088904   0.588220\n"
     ]
    }
   ],
   "source": [
    "!python code/train_target_model.py @args/finetune_avgfp_local.txt --enable_progress_bar false --enable_simple_progress_messages --max_epochs 50 --unfreeze_backbone_at_epoch 25 --uuid examplemodel  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a47d98-54e8-4da2-b8c2-2a175213b717",
   "metadata": {},
   "source": [
    "We can now run inference with this finetuned model using the [inference.py](../code/inference.py) script.\n",
    "\n",
    "| Argument                   | Description                                 | Value                                                             |\n",
    "|:---------------------------|:---------------------------------------------|:------------------------------------------------------------------|\n",
    "| `pretrained_ckpt_path`     | Path to the pretrained model checkpoint     | `output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt` |\n",
    "| `dataset_type`             | Type of dataset being used (rosetta or dms)                 | `dms`                                                             |\n",
    "| `ds_name`                  | Name of the predefined dataset to use       | `avgfp`                                                           |\n",
    "| `encoding`                 | Input encoding method (should be int_seqs for transformer-based METL models)                       | `int_seqs`                                                        |\n",
    "| `predict_mode`             | Prediction mode for inference               | `full_dataset`                                                    |\n",
    "| `batch_size`               | Batch size used during inference            | `512`                                                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "badb204a-b94c-4555-b837-7bf87965fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/metl/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/sg/PycharmProjects/metl/code/datamodules.py:307: UserWarning: Split directory is None for DMSDataModule\n",
      "  warnings.warn(\"Split directory is None for DMSDataModule\")\n",
      "Output directory: output/inference/examplemodel/dms_avgfp\n",
      "Writing predictions to output/inference/examplemodel/dms_avgfp/predictions.npy\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|████████████████| 102/102 [00:39<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "!python code/inference.py --pretrained_ckpt_path=output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt --dataset_type=dms --ds_name=avgfp --encoding=int_seqs --predict_mode full_dataset --batch_size 512 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ca47c2043abe1",
   "metadata": {},
   "source": [
    "# Using your own inference loop\n",
    "If you prefer to have more control and run your own inference loop, we provide easy to use functions to load models and encode data. The [inference.py](../code/inference.py) file contains the functions to load models, and the [encode.py](../code/encode.py) file contains the functions to encode data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "709fda10-c93b-463e-898e-fb22fcaddf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import inference\n",
    "import encode as enc\n",
    "import utils  # for loading dataset metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609c489-43d2-45be-b371-611ebfbb3e12",
   "metadata": {},
   "source": [
    "First, let's load the GFP wild-type sequence and pdb filename from the predefined dataset. This information is necessary to encode variants in the correct format and use models with 3D relative position embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d2007a-cedf-42f0-bf6e-e27a97b4ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = utils.load_dataset_metadata()\n",
    "wt = datasets[\"avgfp\"][\"wt_aa\"]\n",
    "wt_offset = datasets[\"avgfp\"][\"wt_ofs\"]\n",
    "pdb_fn = datasets[\"avgfp\"][\"pdb_fn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc45b4-165c-41d3-be22-f17cc3351a4a",
   "metadata": {},
   "source": [
    "You can load a model using `inference.load_pytorch_module()`. It supports both source and target models, in either PyTorch Lightning's `.ckpt` format or regular PyTorch `.pt` format. You can use keyword arguments to override any hyperparameters that may be stored in the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d44ded2f1139b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sg/PycharmProjects/metl/code/inference.py:159: UserWarning: Transforming checkpoint keys: strip_prefix='model.', add_prefix=''\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "source_model = inference.load_pytorch_module(\"pretrained_models/Hr4GNHws.pt\")\n",
    "# for the target model, use the example model we trained above\n",
    "target_model = inference.load_pytorch_module(\"output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e1e9b-eb44-4888-af78-d46f26c82e0c",
   "metadata": {},
   "source": [
    "You might get a warning about transforming checkpoint keys. This transformation happens automatically in the background. It's necessary when loading a PyTorch Lightning checkpoint because our PyTorch Lightning module wraps the model, and the resulting checkpoint has an additional prefix in the state dictionary keys. If you wanted to, you could fix this by converting the saved checkpoint with [convert_ckpt.py](../code/convert_ckpt.py), but otherwise you can safely ignore this warning. \n",
    "\n",
    "Let's define some variants that we want to feed through the models. Note these variants are 0-based indexing. You can use 1-based indexing if you like, but be sure to specify the correct type of indexing in the encode function below. The default is 0-based indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cfae90-24a5-4183-b57f-c9b5f243c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = [\"E3K,G102S\",\n",
    "            \"T36P,S203T,K207R\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de947f2c-8846-4067-bd9c-b0b27fb5d700",
   "metadata": {},
   "source": [
    "You can encode variants using the `enc.encode()` function. The correct encoding for transformer-based METL models is \"int_seqs\". If you trained a custom model with a different encoding, such as one hot encoding, you would specify that instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5a10fc-6974-4266-9ae0-137be7a25d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_variants = enc.encode(\n",
    "    encoding=\"int_seqs\",\n",
    "    variants=variants,\n",
    "    wt_aa=wt,\n",
    "    wt_offset=wt_offset,\n",
    "    indexing=\"0_indexed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f78c0-0225-4a52-80d3-c624d2c750dc",
   "metadata": {},
   "source": [
    "You can also encode full sequences instead of variants by specifying `char_seqs`. In that case, there would be no need to specify `variants`, `wt_aa`, `wt_offset`, or `indexing`. The cell below shows an example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6386fb-8525-45b2-a488-df286b8bd6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16 11  1 15 17]\n",
      " [11  1  6  8  2]]\n"
     ]
    }
   ],
   "source": [
    "full_seqs = [\"SMART\", \"MAGIC\"]  # sample amino acid sequences\n",
    "encoding_example = enc.encode(encoding=\"int_seqs\", char_seqs=full_seqs)\n",
    "print(encoding_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3070a946-cbea-4cae-9641-09a46911d318",
   "metadata": {},
   "source": [
    "Finally, we can run inference by calling the model with our encoded variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c28dff-ae86-4171-9e9f-f6caf3fec926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2894,  0.1854, -0.5471, -0.0754, -0.3709,  0.1079, -0.4791,  0.2374,\n",
      "          0.1379,  0.9030,  0.3907,  0.5771,  0.3447,  0.3692,  0.4965, -0.4149,\n",
      "          0.1715,  0.1173,  0.1156, -0.2475,  0.0904,  0.1284,  1.1474,  0.8472,\n",
      "          0.3155,  0.5036,  0.5245,  0.4521, -0.8744,  0.2048,  0.5267,  0.5939,\n",
      "         -0.3658, -0.0320, -0.1717,  0.2009,  1.0826, -0.0399,  0.3710,  0.3503,\n",
      "          0.3202,  0.5267, -0.0226,  1.1644, -0.1571, -1.3753,  0.4995, -0.9204,\n",
      "          0.1762,  0.8513,  0.3808, -0.9562, -0.0644, -0.1857, -0.2437],\n",
      "        [-0.1340, -0.3133, -0.8069, -0.0609, -0.0690, -0.5090,  1.5713, -0.6018,\n",
      "          1.5414, -0.5417,  0.0078,  0.2171,  1.4525,  0.1137, -0.0883,  0.8100,\n",
      "          0.1789, -0.1843, -0.2339, -0.0281,  0.0298, -0.2055, -0.5315,  0.9129,\n",
      "          0.0417, -0.1724,  0.2707, -0.7365, -0.2668,  1.3744,  0.5445, -0.0232,\n",
      "          0.0581, -0.1810,  0.0594, -0.4147,  0.4928, -0.0676, -0.9483,  0.2872,\n",
      "          0.3291,  0.5446, -0.8568, -0.2111, -1.2378, -1.4406, -0.4118, -0.5804,\n",
      "         -0.0309, -0.1848, -0.2484, -0.7607, -0.0430, -0.9153, -1.9512]])\n"
     ]
    }
   ],
   "source": [
    "# set model to eval mode\n",
    "source_model.eval()\n",
    "\n",
    "# no need to compute gradients for inference\n",
    "with torch.no_grad():\n",
    "    # note we are specifying the pdb_fn because this model uses 3D RPE\n",
    "    predictions = source_model(torch.tensor(encoded_variants), pdb_fn=pdb_fn)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240cb81-771f-412e-9d23-6ca910cff002",
   "metadata": {},
   "source": [
    "The source model (above) outputs predictions for each Rosetta energy term. The target model (below), which was finetuned on the experimental GFP dataset, outputs a functional score prediction (brightness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a724c16b-2b0f-434a-b402-836cddf681d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4834],\n",
      "        [-0.4877]])\n"
     ]
    }
   ],
   "source": [
    "# set model to eval mode\n",
    "target_model.eval()\n",
    "\n",
    "# no need to compute gradients for inference\n",
    "with torch.no_grad():\n",
    "    # note we are specifying the pdb_fn because this model uses 3D RPE\n",
    "    predictions = target_model(torch.tensor(encoded_variants), pdb_fn=pdb_fn)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd51d71-16ca-44cc-92f5-e4ecb8dbcf24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
