{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9e44b3675389ea",
   "metadata": {},
   "source": [
    "# Inference with METL models\n",
    "This notebook shows how to run inference with METL models trained in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdeaafc4c7596288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2949c7b42a4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# define the name of the project root directory\n",
    "project_root_dir_name = \"metl\"\n",
    "\n",
    "# find the project root by checking each parent directory\n",
    "current_dir = os.getcwd()\n",
    "while os.path.basename(current_dir) != project_root_dir_name and current_dir != os.path.dirname(current_dir):\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# change the current working directory to the project root directory\n",
    "if os.path.basename(current_dir) == project_root_dir_name:\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    print(\"project root directory not found\")\n",
    "    \n",
    "# add the project code folder to the system path so imports work\n",
    "module_path = os.path.abspath(\"code\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64354ce02b142dcd",
   "metadata": {},
   "source": [
    "# Using our inference framework\n",
    "\n",
    "We provide the script [inference.py](../code/inference.py) for running inference with models trained in this repository. It supports similar arguments and datamodule capabilities as used for training the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ba01f-03da-499c-9126-a3ddd7f2692f",
   "metadata": {},
   "source": [
    "The arguements `--write_interval` and `--batch_write_mode` control how often predictions are saved and in what format. \n",
    "\n",
    "The `write_interval` can be set to \"batch\", \"epoch\", or \"batch_and_epoch\". When set to \"batch\", predictions will be saved to disk after each batch. When set to \"epoch\", predictions will first be stored in RAM until all data has been processed, and then they will be written to disk. If you have a lot of data which might not fit in RAM, then you will want to set `--write_interval` to \"batch\" (default).\n",
    "\n",
    "The `--batch_write_mode` can be set to \"combined_csv\", \"separate_csv\", or \"separate_npy\". When set to \"combined_csv\", there will be a single output csv file, and it will be appended to after each batch is processed. When set to either \"separate_csv\" or \"separate_npy\", there will be a separate output file for each batch in either .csv or .npy format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014acc1-af5e-4c3b-9a0d-d0a69976c772",
   "metadata": {},
   "source": [
    "## Source model example\n",
    "This repository contains a sample GFP Rosetta dataset and a pretrained METL-Local GFP source model, which we can use as examples. \n",
    "\n",
    "We specify the following arguments:\n",
    "\n",
    "| Argument               | Description                                                | Value                                      |\n",
    "|:------------------------|:------------------------------------------------------------|:--------------------------------------------|\n",
    "| `pretrained_ckpt_path` | Path to the pretrained model checkpoint                    | `pretrained_models/Hr4GNHws.pt`            |\n",
    "| `dataset_type`         | Type of dataset being used (rosetta or dms)                                | `rosetta`                                  |\n",
    "| `ds_fn`                | Path to the database file for the dataset                  | `data/rosetta_data/avgfp/avgfp.db`         |\n",
    "| `batch_size`           | Batch size used during inference               | `512`                                     |\n",
    "\n",
    "The inference script will automatically save output in the `output/inference` directory. There will be an output csv file for each processed batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5160d8ea88e32c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sg/PycharmProjects/metl/code/inference.py:78: UserWarning: Prefixing checkpoint keys with 'model.' to match model format.\n",
      "  warnings.warn(f\"Prefixing checkpoint keys with '{prefix}' to match model format.\")\n",
      "Using example_input_array with pdb_fn='1gfl_cm.pdb' and aa_seq_len=237\n",
      "Output directory: output/inference/Hr4GNHws/rosetta_avgfp/full_dataset\n",
      "Writing predictions to output/inference/Hr4GNHws/rosetta_avgfp/full_dataset/predictions.npy\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████████████| 20/20 [00:06<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "!python code/inference.py --pretrained_ckpt_path=pretrained_models/Hr4GNHws.pt --dataset_type=rosetta --ds_fn=data/rosetta_data/avgfp/avgfp.db --batch_size=512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415b48e-8c9c-4d86-b5c3-362374dbc875",
   "metadata": {},
   "source": [
    "By default, the script will compute predictions for the full dataset. If you only need to save predictions for a particular train, validation, or test set, you can do so by setting the `--split_dir` and `--predict_mode` arguments. The function call below will compute predictions just for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf7d281-4293-46df-b6e9-b49ebcf5b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sg/PycharmProjects/metl/code/inference.py:78: UserWarning: Prefixing checkpoint keys with 'model.' to match model format.\n",
      "  warnings.warn(f\"Prefixing checkpoint keys with '{prefix}' to match model format.\")\n",
      "Using example_input_array with pdb_fn='1gfl_cm.pdb' and aa_seq_len=237\n",
      "Output directory: output/inference/Hr4GNHws/rosetta_avgfp/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991/test\n",
      "Writing predictions to output/inference/Hr4GNHws/rosetta_avgfp/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991/test/predictions.npy\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|████████████████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "!python code/inference.py --pretrained_ckpt_path=pretrained_models/Hr4GNHws.pt --dataset_type=rosetta --ds_fn=data/rosetta_data/avgfp/avgfp.db --batch_size=512 --split_dir=data/rosetta_data/avgfp/splits/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991 --predict_mode=test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610a8a3-0353-43c2-935a-9aa53f9fdff7",
   "metadata": {},
   "source": [
    "## Target (finetuned) model example\n",
    "We first need to finetune a model using experimental data. Run the command below, which will finetune the pretrained model above using the GFP experimental dataset. Note we manually specify the UUID `examplemodel` for this model. See the [finetuning.ipynb](finetuning.ipynb) notebook for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6ee595-c810-404f-b639-6a513d98d461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed not specified, using: 233751893\n",
      "Global seed set to 233751893\n",
      "User gave model UUID: examplemodel\n",
      "Did not find existing log directory corresponding to given UUID: examplemodel\n",
      "Created log directory: output/training_logs/examplemodel\n",
      "Final UUID: examplemodel\n",
      "Final log directory: output/training_logs/examplemodel\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/metl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Number of training steps is 50\n",
      "Number of warmup steps is 0.5\n",
      "Second warmup phase starts at step 25\n",
      "total_steps 50\n",
      "phase1_total_steps 25\n",
      "phase2_total_steps 25\n",
      "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m  In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m Out sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model                  │ TransferMod… │  2.4 M │\u001b[37m \u001b[0m\u001b[37m[128, 237]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [128, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ model.model            │ SequentialW… │  2.4 M │\u001b[37m \u001b[0m\u001b[37m[128, 237]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [128, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ model.model.backbone   │ SequentialW… │  2.4 M │\u001b[37m \u001b[0m\u001b[37m[128, 237]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ model.model.dropout    │ Dropout      │      0 │\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ model.model.flatten    │ Flatten      │      0 │\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ model.model.prediction │ Linear       │    257 │\u001b[37m \u001b[0m\u001b[37m[128, 256]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m  [128, 1]\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ test_pearson           │ PearsonCorr… │      0 │\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\n",
      "│\u001b[2m \u001b[0m\u001b[2m7\u001b[0m\u001b[2m \u001b[0m│ test_spearman          │ SpearmanCor… │      0 │\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m         ?\u001b[0m\u001b[37m \u001b[0m│\n",
      "└───┴────────────────────────┴──────────────┴────────┴────────────┴────────────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 257                                                           \n",
      "\u001b[1mNon-trainable params\u001b[0m: 2.4 M                                                     \n",
      "\u001b[1mTotal params\u001b[0m: 2.4 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 9                                       \n",
      "Starting sanity check...\n",
      "Sanity check complete.\n",
      "Starting training...\n",
      "Epoch     0: Train Loss =   2.651, Val Loss =   2.353\n",
      "Epoch     1: Train Loss =   2.590, Val Loss =   2.301\n",
      "Epoch     2: Train Loss =   2.547, Val Loss =   2.251\n",
      "Epoch     3: Train Loss =   2.487, Val Loss =   2.203\n",
      "Epoch     4: Train Loss =   2.412, Val Loss =   2.156\n",
      "Epoch     5: Train Loss =   2.341, Val Loss =   2.113\n",
      "Epoch     6: Train Loss =   2.382, Val Loss =   2.071\n",
      "Epoch     7: Train Loss =   2.274, Val Loss =   2.033\n",
      "Epoch     8: Train Loss =   2.227, Val Loss =   1.997\n",
      "Epoch     9: Train Loss =   2.187, Val Loss =   1.965\n",
      "Epoch    10: Train Loss =   2.161, Val Loss =   1.935\n",
      "Epoch    11: Train Loss =   2.210, Val Loss =   1.909\n",
      "Epoch    12: Train Loss =   2.124, Val Loss =   1.886\n",
      "Epoch    13: Train Loss =   2.078, Val Loss =   1.865\n",
      "Epoch    14: Train Loss =   2.082, Val Loss =   1.848\n",
      "Epoch    15: Train Loss =   2.078, Val Loss =   1.833\n",
      "Epoch    16: Train Loss =   2.026, Val Loss =   1.821\n",
      "Epoch    17: Train Loss =   2.049, Val Loss =   1.811\n",
      "Epoch    18: Train Loss =   2.027, Val Loss =   1.803\n",
      "Epoch    19: Train Loss =   1.977, Val Loss =   1.797\n",
      "Epoch    20: Train Loss =   2.008, Val Loss =   1.793\n",
      "Epoch    21: Train Loss =   1.980, Val Loss =   1.791\n",
      "Epoch    22: Train Loss =   2.044, Val Loss =   1.789\n",
      "Epoch    23: Train Loss =   1.946, Val Loss =   1.789\n",
      "Epoch    24: Train Loss =   1.983, Val Loss =   1.789\n",
      "Epoch    25: Train Loss =   1.965, Val Loss =   1.789\n",
      "Epoch    26: Train Loss =   2.020, Val Loss =   1.761\n",
      "Epoch    27: Train Loss =   1.984, Val Loss =   1.735\n",
      "Epoch    28: Train Loss =   1.954, Val Loss =   1.708\n",
      "Epoch    29: Train Loss =   1.866, Val Loss =   1.682\n",
      "Epoch    30: Train Loss =   1.825, Val Loss =   1.656\n",
      "Epoch    31: Train Loss =   1.828, Val Loss =   1.631\n",
      "Epoch    32: Train Loss =   1.818, Val Loss =   1.607\n",
      "Epoch    33: Train Loss =   1.726, Val Loss =   1.583\n",
      "Epoch    34: Train Loss =   1.736, Val Loss =   1.560\n",
      "Epoch    35: Train Loss =   1.661, Val Loss =   1.539\n",
      "Epoch    36: Train Loss =   1.635, Val Loss =   1.518\n",
      "Epoch    37: Train Loss =   1.631, Val Loss =   1.499\n",
      "Epoch    38: Train Loss =   1.677, Val Loss =   1.482\n",
      "Epoch    39: Train Loss =   1.604, Val Loss =   1.466\n",
      "Epoch    40: Train Loss =   1.534, Val Loss =   1.452\n",
      "Epoch    41: Train Loss =   1.550, Val Loss =   1.440\n",
      "Epoch    42: Train Loss =   1.516, Val Loss =   1.429\n",
      "Epoch    43: Train Loss =   1.476, Val Loss =   1.421\n",
      "Epoch    44: Train Loss =   1.455, Val Loss =   1.414\n",
      "Epoch    45: Train Loss =   1.451, Val Loss =   1.410\n",
      "Epoch    46: Train Loss =   1.467, Val Loss =   1.407\n",
      "Epoch    47: Train Loss =   1.412, Val Loss =   1.405\n",
      "Epoch    48: Train Loss =   1.438, Val Loss =   1.404\n",
      "Epoch    49: Train Loss =   1.458, Val Loss =   1.404\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "Restoring states from the checkpoint path at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Loaded model weights from checkpoint at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Starting testing...\n",
      "Testing complete.\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.411665678024292    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_pearson       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6630585789680481    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_spearman      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.639276385307312    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "Restoring states from the checkpoint path at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Loaded model weights from checkpoint at output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt\n",
      "Starting prediction...\n",
      "Prediction complete.\n",
      "saving a scatter plot for set: train (128 variants)\n",
      "saving a scatter plot for set: val (32 variants)\n",
      "saving a scatter plot for set: test (4655 variants)\n",
      "            mse  pearsonr        r2  spearmanr\n",
      "set                                           \n",
      "train  1.546191  0.665460 -0.325300   0.659309\n",
      "val    1.404264  0.633410 -0.302175   0.656402\n",
      "test   1.411665  0.663058 -0.268134   0.639276\n"
     ]
    }
   ],
   "source": [
    "!python code/train_target_model.py @args/finetune_avgfp_local.txt --enable_progress_bar false --enable_simple_progress_messages --max_epochs 50 --unfreeze_backbone_at_epoch 25 --uuid examplemodel  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a47d98-54e8-4da2-b8c2-2a175213b717",
   "metadata": {},
   "source": [
    "We can now run inference with this finetuned model using the [inference.py](../code/inference.py) script.\n",
    "\n",
    "| Argument                   | Description                                 | Value                                                             |\n",
    "|:---------------------------|:---------------------------------------------|:------------------------------------------------------------------|\n",
    "| `pretrained_ckpt_path`     | Path to the pretrained model checkpoint     | `output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt` |\n",
    "| `dataset_type`             | Type of dataset being used (rosetta or dms)                 | `dms`                                                             |\n",
    "| `ds_name`                  | Name of the predefined dataset to use       | `avgfp`                                                           |\n",
    "| `encoding`                 | Input encoding method (should be int_seqs for transformer-based METL models)                       | `int_seqs`                                                        |\n",
    "| `predict_mode`             | Prediction mode for inference               | `full_dataset`                                                    |\n",
    "| `batch_size`               | Batch size used during inference            | `512`                                                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "badb204a-b94c-4555-b837-7bf87965fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/metl/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/sg/PycharmProjects/metl/code/datamodules.py:307: UserWarning: Split directory is None for DMSDataModule\n",
      "  warnings.warn(\"Split directory is None for DMSDataModule\")\n",
      "Output directory: output/inference/examplemodel/dms_avgfp\n",
      "Writing predictions to output/inference/examplemodel/dms_avgfp/predictions.npy\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|████████████████| 102/102 [00:30<00:00,  3.30it/s]\n"
     ]
    }
   ],
   "source": [
    "!python code/inference.py --pretrained_ckpt_path=output/training_logs/examplemodel/checkpoints/epoch=49-step=50.ckpt --dataset_type=dms --ds_name=avgfp --encoding=int_seqs --predict_mode full_dataset --batch_size 512 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ca47c2043abe1",
   "metadata": {},
   "source": [
    "# Using your own inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44ded2f1139b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
