{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30ea18e-6b5a-47d4-b7a4-1330804b5602",
   "metadata": {},
   "source": [
    "# Finetune on experimental data\n",
    "This notebook demonstrates how to finetune METL models on experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432eebaf-00b8-42bf-b927-fd651e6ab94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T22:51:36.573559Z",
     "start_time": "2024-02-16T22:51:36.569490Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c566507e-1012-4415-82ba-7498950e0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# define the name of the project root directory\n",
    "project_root_dir_name = \"metl\"\n",
    "\n",
    "# find the project root by checking each parent directory\n",
    "current_dir = os.getcwd()\n",
    "while os.path.basename(current_dir) != project_root_dir_name and current_dir != os.path.dirname(current_dir):\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# change the current working directory to the project root directory\n",
    "if os.path.basename(current_dir) == project_root_dir_name:\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    print(\"project root directory not found\")\n",
    "    \n",
    "# add the project code folder to the system path so imports work\n",
    "module_path = os.path.abspath(\"code\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19876208-66f9-46b5-8f50-8e798fa815a4",
   "metadata": {},
   "source": [
    "# Acquire an experimental dataset\n",
    "\n",
    "For demonstration purposes, this repository contains the [avGFP dataset](../data/dms_data/avgfp) from [Sarkisyan et al. (2016)](https://doi.org/10.1038/nature17995). \n",
    "See the [metl-pub](https://github.com/gitter-lab/metl-pub) repository to access the other experimental datasets we used in our preprint.\n",
    "See the README in the [dms_data](../data/dms_data) directory for information about how to use your own experimental dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abf8b1-aa2d-4055-9184-d962ba0d4582",
   "metadata": {},
   "source": [
    "# Acquire a pretrained model\n",
    "Pretrained METL models are available in the [metl-pretrained](https://github.com/gitter-lab/metl-pretrained) repository. You can use one of those, or you can pretrain your own METL model (see [pretraining.ipynb](pretraining.ipynb)). \n",
    "\n",
    "For demonstration purposes, we include a pretrained avGFP METL-Local model from the [metl-pretrained](https://github.com/gitter-lab/metl-pretrained) repository in the [pretrained_models](../pretrained_models) directory. This model is `METL-L-2M-3D-GFP` (UUID: `Hr4GNHws`).\n",
    "It is the avGFP METL-Local source model we used for the analysis in our preprint.\n",
    "\n",
    "We will show how to finetune this model using the [experimental avGFP dataset](../data/dms_data/avgfp)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a30235-357a-4326-a4ff-77ab26eb5d7f",
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "\n",
    "The script for finetuning on experimental data is [train_target_model.py](train_target_model.py). This script has a number of arguments you can view by uncommenting and running the below cell. There are additional arguments related to architecture that won't show up if you run the command, but you can view them in [models.py](../code/models.py) in the `TransferModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca8aeea-3dc3-47eb-915c-d80132be8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python code/train_target_model.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8c31b-2da2-4ba7-9f4e-39e30dce8056",
   "metadata": {},
   "source": [
    "We set up finetuning arguments for this example in [finetune_avgfp_local.txt](../args/pretrain_avgfp_local.txt) in the [args](../args) directory. This argument file can be used directly with [train_target_model.py](train_target_model.py) by calling the command `!python code/train_target_model.py @args/finetune_avgfp_local.txt` (we do this in the next section).\n",
    "\n",
    "Uncomment and run the cell below to view the contents of the argument file. The sections below will walk through and explain the key arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06a897f-877d-4e41-9bee-4d3eabeead7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"args/finetune_avgfp_local.txt\", \"r\") as file:\n",
    "#     contents = file.read()\n",
    "#     print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2610124-fa2c-4709-98fc-bae51b258338",
   "metadata": {},
   "source": [
    "## Dataset arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56ee90-90be-41fa-bc99-c13f94e14976",
   "metadata": {},
   "source": [
    "\n",
    "Specify the dataset name and the train/val/test split. The dataset must be defined in [datasets.yml](../data/dms_data/datasets.yml). For demonstration purposes, we are using one of the reduced dataset size splits with a dataset size of 160 (train size of 128).\n",
    "```\n",
    "--ds_name\n",
    "avgfp\n",
    "--split_dir\n",
    "data/dms_data/avgfp/splits/resampled/resampled_ds160_val0.2_te0.1_w1abc2f4e9a64_s1_r8099/resampled_ds160_val0.2_te0.1_w1abc2f4e9a64_s1_r8099_rep_0\n",
    "```\n",
    "\n",
    "Specify the names of the train, validation, and test set files in the split directory. Using \"auto\" for the test_name will select the super test set (\"stest.txt\") if it exists in the split directory, otherwise it will use the standard test set (\"test.txt\").\n",
    "\n",
    "```\n",
    "--train_name\n",
    "train\n",
    "--val_name\n",
    "val\n",
    "--test_name\n",
    "test\n",
    "```\n",
    "\n",
    "The name of the target column in the dataset dataframe. The model will be finetuned to predict the score in this column.\n",
    "\n",
    "```\n",
    "--target_names\n",
    "score\n",
    "```\n",
    "\n",
    "The METL-Local model we are finetuning uses 3D structure-based relative position embeddings, so we need to specify the PDB filename. This PDB file is in the [data/pdb_files](../data/pdb_files) directory, which the script checks by default, so there is no need to specify the full path. You can also just specify \"auto\" to use the PDB file defined for this dataset in [datasets.yml](../data/dms_data/datasets.yml).\n",
    "\n",
    "```\n",
    "--pdb_fn\n",
    "1gfl_cm.pdb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cea13-feae-4e54-bf0f-dcbe97f4409f",
   "metadata": {},
   "source": [
    "## Network architecture arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee9762-cae7-4e21-8435-f6dd49781b8c",
   "metadata": {},
   "source": [
    "For finetuning, we implemented a special model `transfer_model` that handles pretrained checkpoints with top nets. \n",
    "```\n",
    "--model_name\n",
    "transfer_model\n",
    "```\n",
    "\n",
    "The pretrained checkpoint can be a PyTorch checkpoint (.pt file) downloaded from the [metl-pretrained](https://github.com/gitter-lab/metl-pretrained) repository or a PyTorch Lightning checkpoint (.ckpt file) obtained from pretraining a model with this repository.\n",
    "```\n",
    "--pretrained_ckpt_path\n",
    "pretrained_models/Hr4GNHws.pt\n",
    "```\n",
    "\n",
    "The backbone cutoff determines where to cutoff the pretrained model and place the new prediction head. For METL-Local models, we recommend backbone cutoff -1, and for METL-Global models we recommend backbone cutoff -2. \n",
    "\n",
    "```\n",
    "--backbone_cutoff\n",
    "-1\n",
    "```\n",
    "\n",
    "The remaining arguments determine the encoding, which should be set to `int_seqs`, whether to use dropout after the backbone cutoff, and the architecture of the new top net. You can leave these values as-is to match what we did for the preprint.\n",
    "\n",
    "```\n",
    "--encoding\n",
    "int_seqs\n",
    "--dropout_after_backbone\n",
    "--dropout_after_backbone_rate\n",
    "0.5\n",
    "--top_net_type\n",
    "linear\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94c112-9770-4a5f-93e0-acf4d9acae16",
   "metadata": {},
   "source": [
    "## Finetuning strategy arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb96cb6-7815-4efa-9b6f-305df9bb3050",
   "metadata": {},
   "source": [
    "We implemented a dual-phase finetuning strategy. During the first phase, the backbone weights are frozen and only the top net is trained. During the second phase, all the network weights are unfrozen and trained at a reduced learning rate.\n",
    "\n",
    "The unfreeze_backbone_at_epoch argument determines the training epoch at which to unfreeze the backbone. We train the models for 500 epochs, so the backbone is unfrozen halfway through at epoch 250.\n",
    "\n",
    "```\n",
    "--finetuning\n",
    "--finetuning_strategy\n",
    "backbone\n",
    "--unfreeze_backbone_at_epoch\n",
    "250\n",
    "--backbone_always_align_lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d4584-a0ce-45c3-8fb7-8c34d3a984c3",
   "metadata": {},
   "source": [
    "## Optimization arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d10e8-21f3-4b9e-8134-99cb053bef13",
   "metadata": {},
   "source": [
    "Basic optimizer arguments include the batch size, learning rate, and maximum number of epochs to train for. Unless early stopping is enabled, the model will train for the given number of epochs. \n",
    "\n",
    "```\n",
    "--optimizer\n",
    "adamw\n",
    "--weight_decay\n",
    "0.1\n",
    "--batch_size\n",
    "128\n",
    "--learning_rate\n",
    "0.001\n",
    "--max_epochs\n",
    "500\n",
    "--gradient_clip_val\n",
    "0.5\n",
    "```\n",
    "\n",
    "The learning rate scheduler we used for finetuning is a dual phase learning rate schedule that matches the dual phase finetuning strategy. Each phase has a linear learning rate warmup for 1% of the total steps in that phase. There is also a cosine decay for the learning rate for each phase. The phase 2 learning rate is 10% of the phase 1 learning rate.\n",
    "\n",
    "```\n",
    "--lr_scheduler\n",
    "dual_phase_warmup_constant_cosine_decay\n",
    "--warmup_steps\n",
    ".01\n",
    "--phase2_lr_ratio\n",
    "0.1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Arguments\n",
    "\n",
    "<u>Default</u>\n",
    "\n",
    "By default, the loss function is **mean squared error (MSE)**:\n",
    "```\n",
    "--loss_func\n",
    "mse\n",
    "```\n",
    "This does not need to be explicitly set in an argument file, but including it can aid reproducibility and help track different experimental runs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Using Ordinal-Specific Loss\n",
    "\n",
    "All ordinal loss functions require the target column for DMS finetuing to have labels 0, 1, ..., N-1 for N ordered classes, where 0 is the lowest and N-1 is the highest. If you need custom labels and want to do ordinal specific loss feel free to raise an issue on github.\n",
    "\n",
    "<u>CORN</u>\n",
    "\n",
    "If you are working with **ordinal data** (e.g., labels 0, 1, ..., N-1 for N ordered classes), consider using the ordinal-specific loss function (corn):\n",
    "\n",
    "```\n",
    "--loss_func\n",
    "corn\n",
    "--num_classes\n",
    "4\n",
    "--top_net_output_dim\n",
    "3\n",
    "--corn_coral_log_feature\n",
    "3\n",
    "```\n",
    "\n",
    "This implements the CORN loss described in [this paper](https://arxiv.org/abs/2111.08851). The corn specific functions in *metl* are adapted from the [CORN+CORAL library](https://github.com/Raschka-research-group/coral-pytorch/tree/main).\n",
    "\n",
    "- `--num_classes` specifies the number of classes (as an int).\n",
    "- `--top_net_output_dim` output dimension from the final layer, must be number of classes minus 1 (as an int), to correspond with each probability task.\n",
    "- `--corn_coral_log_feature` specifies which predicted class probability should be used in *metl*'s logging. This does **not** affect the final saved predictions, the probabilities of being in any class is saved, only the logged test metrics and plots.\n",
    "  - Options: \"raw_predictions\", 2,..,N, where N is the maximum number of classes.\n",
    "  - Note: `1` is not allowed, as the probability a variant is in the lowest class is always equal to 1.\n",
    "  - For example, `3` logs the probability of the sample being in class 3.\n",
    "  - By default, `corn_coral_log_feature` is set to `raw_predictions`, which converts class probabilities into the highest class with probability greater than 0.5.\n",
    "\n",
    "\n",
    "\n",
    "<u>CORAL</u>\n",
    "This will implement the loss and final layer described in [this paper](https://doi.org/10.1016/j.patrec.2020.11.008). The coral specific functions in *metl* are adapted from the [CORN+CORAL library](https://github.com/Raschka-research-group/coral-pytorch/tree/main).\n",
    "```\n",
    "--loss_func\n",
    "coral\n",
    "--top_net_type\n",
    "coral\n",
    "--preinit_bias\n",
    "--num_classes\n",
    "4\n",
    "--corn_coral_log_feature\n",
    "3\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<u> CLASS IMBALANCE </u>\n",
    "\n",
    "If you have ordinal data which has class imbalance, you could consider trying to weight the loss function to pay a greater penalty during training to classes which are not as represented using the flag:\n",
    "\n",
    "```\n",
    "--use_importance_weights\n",
    "```\n",
    "\n",
    "This will calculate the ratio of each class in the training set and weight them in the loss by the inverse of this ratio (if any class has zero samples the value of 0.1 is assigned to stop divide by zero errors.) The `--use_importance_weights` flag can be used with `--loss_func mse`, `--loss_func corn`, or `--loss_func coral`.\n",
    "\n",
    "Alternatively, you can set your own weights using the flag\n",
    "```\n",
    "--set_importance_weights\n",
    "0.3\n",
    "0.3\n",
    "0.4\n",
    "```\n",
    "\n",
    "If you are using the CORN or CORAL loss, what is weighted is each probability task, since there are only N-1 probability tasks, there must be N-1 weights (where N is the number of classes). For MSE loss N weights must be specified.\n",
    "\n",
    "\n",
    "<u>MULTIPLE EXPERIMENTAL BATCHES</u>\n",
    "\n",
    "\n",
    "\n",
    "or  . Other parameters exist to change the behavior of the loss functions,\n",
    "such as `--use_importance_weights` which weights the proportion of each class equally to the loss function.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging arguments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have built in functionality for tracking model training with Weights & Biases. If you have a Weights and Biases account, set the environment variable `WANDB_API_KEY` to your API key and set the flag `--use_wandb` instead of `--no_use_wandb` below.\n",
    "\n",
    "```\n",
    "--no_use_wandb\n",
    "--wandb_project\n",
    "metl-target\n",
    "--wandb_online\n",
    "--experiment\n",
    "default\n",
    "```\n",
    "\n",
    "The below argument determines where to place the log directory locally.\n",
    "```\n",
    "--log_dir_base\n",
    "output/training_logs\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "53a2fda3-6dfa-46d5-ad3d-3055eda0b29a",
   "metadata": {},
   "source": [
    "# Running training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d8d23-9d54-4888-842d-4fc8fd843b40",
   "metadata": {},
   "source": [
    "All the arguments described above are contained in [finetune_avgfp_local.txt](../args/pretrain_avgfp_local.txt), which can be fed directly into [train_target_model.py](train_target_model.py).\n",
    "\n",
    "PyTorch Lightning has a built-in progress bar that is convenient for seeing training progress, but it does not display correctly in Jupyter when calling the script with `!python`. We are going to disable the progress bar for by setting the flag `--enable_progress_bar false`. Instead, we implemented a simple print statement to track training progress, which we will enable with the flag `--enable_simple_progress_messages`. \n",
    "\n",
    "The [train_target_model.py](../code/train_target_model.py) script can support running on Apple Silicon with acceleration via MPS, but the version of PyTorch used in this environment is slightly outdated and does not support all MPS operations, so MPS support has been disabled. The script will run on GPU via CUDA if available, otherwise it will use CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977b4d8d-4662-4e03-955c-dc4a8ae7c1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed not specified, using: 979104742\n",
      "Global seed set to 979104742\n",
      "User gave model UUID: inqx2jYi\n",
      "Did not find existing log directory corresponding to given UUID: inqx2jYi\n",
      "Created log directory: output/training_logs/inqx2jYi\n",
      "Final UUID: inqx2jYi\n",
      "Final log directory: output/training_logs/inqx2jYi\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/metl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1892: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Number of training steps is 50\n",
      "Number of warmup steps is 0.5\n",
      "Second warmup phase starts at step 25\n",
      "total_steps 50\n",
      "phase1_total_steps 25\n",
      "phase2_total_steps 25\n",
      "┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
      "┃\u001B[1;35m \u001B[0m\u001B[1;35m \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mName                  \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mType        \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mParams\u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35m  In sizes\u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35m Out sizes\u001B[0m\u001B[1;35m \u001B[0m┃\n",
      "┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
      "│\u001B[2m \u001B[0m\u001B[2m0\u001B[0m\u001B[2m \u001B[0m│ model                  │ TransferMod… │  2.4 M │\u001B[37m \u001B[0m\u001B[37m[128, 237]\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m  [128, 1]\u001B[0m\u001B[37m \u001B[0m│\n",
      "│\u001B[2m \u001B[0m\u001B[2m1\u001B[0m\u001B[2m \u001B[0m│ model.model            │ SequentialW… │  2.4 M │\u001B[37m \u001B[0m\u001B[37m[128, 237]\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m  [128, 1]\u001B[0m\u001B[37m \u001B[0m│\n",
      "│\u001B[2m \u001B[0m\u001B[2m2\u001B[0m\u001B[2m \u001B[0m│ model.model.backbone   │ SequentialW… │  2.4 M │\u001B[37m \u001B[0m\u001B[37m[128, 237]\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m[128, 256]\u001B[0m\u001B[37m \u001B[0m│\n",
      "│\u001B[2m \u001B[0m\u001B[2m3\u001B[0m\u001B[2m \u001B[0m│ model.model.dropout    │ Dropout      │      0 │\u001B[37m \u001B[0m\u001B[37m[128, 256]\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m[128, 256]\u001B[0m\u001B[37m \u001B[0m│\n",
      "│\u001B[2m \u001B[0m\u001B[2m4\u001B[0m\u001B[2m \u001B[0m│ model.model.flatten    │ Flatten      │      0 │\u001B[37m \u001B[0m\u001B[37m[128, 256]\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m[128, 256]\u001B[0m\u001B[37m \u001B[0m│\n",
      "│\u001B[2m \u001B[0m\u001B[2m5\u001B[0m\u001B[2m \u001B[0m│ model.model.prediction │ Linear       │    257 │\u001B[37m \u001B[0m\u001B[37m[128, 256]\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m  [128, 1]\u001B[0m\u001B[37m \u001B[0m│\n",
      "│\u001B[2m \u001B[0m\u001B[2m6\u001B[0m\u001B[2m \u001B[0m│ test_pearson           │ PearsonCorr… │      0 │\u001B[37m \u001B[0m\u001B[37m         ?\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m         ?\u001B[0m\u001B[37m \u001B[0m│\n",
      "│\u001B[2m \u001B[0m\u001B[2m7\u001B[0m\u001B[2m \u001B[0m│ test_spearman          │ SpearmanCor… │      0 │\u001B[37m \u001B[0m\u001B[37m         ?\u001B[0m\u001B[37m \u001B[0m│\u001B[37m \u001B[0m\u001B[37m         ?\u001B[0m\u001B[37m \u001B[0m│\n",
      "└───┴────────────────────────┴──────────────┴────────┴────────────┴────────────┘\n",
      "\u001B[1mTrainable params\u001B[0m: 257                                                           \n",
      "\u001B[1mNon-trainable params\u001B[0m: 2.4 M                                                     \n",
      "\u001B[1mTotal params\u001B[0m: 2.4 M                                                             \n",
      "\u001B[1mTotal estimated model params size (MB)\u001B[0m: 9                                       \n",
      "Starting sanity check...\n",
      "Sanity check complete.\n",
      "Starting training...\n",
      "Epoch     0: Train Loss =   2.587, Val Loss =   2.395\n",
      "Epoch     1: Train Loss =   2.663, Val Loss =   2.343\n",
      "Epoch     2: Train Loss =   2.645, Val Loss =   2.293\n",
      "Epoch     3: Train Loss =   2.571, Val Loss =   2.245\n",
      "Epoch     4: Train Loss =   2.518, Val Loss =   2.199\n",
      "Epoch     5: Train Loss =   2.440, Val Loss =   2.155\n",
      "Epoch     6: Train Loss =   2.381, Val Loss =   2.114\n",
      "Epoch     7: Train Loss =   2.322, Val Loss =   2.076\n",
      "Epoch     8: Train Loss =   2.298, Val Loss =   2.040\n",
      "Epoch     9: Train Loss =   2.353, Val Loss =   2.008\n",
      "Epoch    10: Train Loss =   2.216, Val Loss =   1.978\n",
      "Epoch    11: Train Loss =   2.162, Val Loss =   1.952\n",
      "Epoch    12: Train Loss =   2.203, Val Loss =   1.928\n",
      "Epoch    13: Train Loss =   2.136, Val Loss =   1.908\n",
      "Epoch    14: Train Loss =   2.160, Val Loss =   1.890\n",
      "Epoch    15: Train Loss =   2.125, Val Loss =   1.876\n",
      "Epoch    16: Train Loss =   2.145, Val Loss =   1.863\n",
      "Epoch    17: Train Loss =   2.055, Val Loss =   1.854\n",
      "Epoch    18: Train Loss =   2.090, Val Loss =   1.846\n",
      "Epoch    19: Train Loss =   2.050, Val Loss =   1.840\n",
      "Epoch    20: Train Loss =   2.081, Val Loss =   1.836\n",
      "Epoch    21: Train Loss =   2.055, Val Loss =   1.834\n",
      "Epoch    22: Train Loss =   2.053, Val Loss =   1.832\n",
      "Epoch    23: Train Loss =   2.028, Val Loss =   1.831\n",
      "Epoch    24: Train Loss =   2.068, Val Loss =   1.831\n",
      "Epoch    25: Train Loss =   2.103, Val Loss =   1.831\n",
      "Epoch    26: Train Loss =   2.082, Val Loss =   1.804\n",
      "Epoch    27: Train Loss =   2.039, Val Loss =   1.777\n",
      "Epoch    28: Train Loss =   1.993, Val Loss =   1.751\n",
      "Epoch    29: Train Loss =   2.013, Val Loss =   1.725\n",
      "Epoch    30: Train Loss =   1.950, Val Loss =   1.700\n",
      "Epoch    31: Train Loss =   1.893, Val Loss =   1.677\n",
      "Epoch    32: Train Loss =   1.879, Val Loss =   1.654\n",
      "Epoch    33: Train Loss =   1.818, Val Loss =   1.632\n",
      "Epoch    34: Train Loss =   1.757, Val Loss =   1.612\n",
      "Epoch    35: Train Loss =   1.765, Val Loss =   1.593\n",
      "Epoch    36: Train Loss =   1.776, Val Loss =   1.576\n",
      "Epoch    37: Train Loss =   1.717, Val Loss =   1.561\n",
      "Epoch    38: Train Loss =   1.672, Val Loss =   1.546\n",
      "Epoch    39: Train Loss =   1.667, Val Loss =   1.534\n",
      "Epoch    40: Train Loss =   1.708, Val Loss =   1.523\n",
      "Epoch    41: Train Loss =   1.628, Val Loss =   1.514\n",
      "Epoch    42: Train Loss =   1.641, Val Loss =   1.506\n",
      "Epoch    43: Train Loss =   1.702, Val Loss =   1.500\n",
      "Epoch    44: Train Loss =   1.541, Val Loss =   1.496\n",
      "Epoch    45: Train Loss =   1.598, Val Loss =   1.493\n",
      "Epoch    46: Train Loss =   1.558, Val Loss =   1.491\n",
      "Epoch    47: Train Loss =   1.604, Val Loss =   1.489\n",
      "Epoch    48: Train Loss =   1.630, Val Loss =   1.489\n",
      "Epoch    49: Train Loss =   1.562, Val Loss =   1.489\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "Restoring states from the checkpoint path at output/training_logs/inqx2jYi/checkpoints/epoch=49-step=50.ckpt\n",
      "Loaded model weights from checkpoint at output/training_logs/inqx2jYi/checkpoints/epoch=49-step=50.ckpt\n",
      "Starting testing...\n",
      "Testing complete.\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   1.5082340240478516    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m      test_pearson       \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.4386957883834839    \u001B[0m\u001B[35m \u001B[0m│\n",
      "│\u001B[36m \u001B[0m\u001B[36m      test_spearman      \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.4137117862701416    \u001B[0m\u001B[35m \u001B[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "Restoring states from the checkpoint path at output/training_logs/inqx2jYi/checkpoints/epoch=49-step=50.ckpt\n",
      "Loaded model weights from checkpoint at output/training_logs/inqx2jYi/checkpoints/epoch=49-step=50.ckpt\n",
      "Starting prediction...\n",
      "Prediction complete.\n",
      "saving a scatter plot for set: train (128 variants)\n",
      "saving a scatter plot for set: val (32 variants)\n",
      "saving a scatter plot for set: test (4655 variants)\n",
      "            mse  pearsonr        r2  spearmanr\n",
      "set                                           \n",
      "train  1.630327  0.489046 -0.397417   0.439851\n",
      "val    1.488715  0.481981 -0.380486   0.524608\n",
      "test   1.508234  0.438696 -0.354884   0.413712\n"
     ]
    }
   ],
   "source": [
    "!python code/train_target_model.py @args/finetune_avgfp_local.txt --enable_progress_bar false --enable_simple_progress_messages --max_epochs 50 --unfreeze_backbone_at_epoch 25 --uuid inqx2jYi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fc407-6ab1-45e3-8e6a-9b717dca7f00",
   "metadata": {},
   "source": [
    "# Additional recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8e0e5-8bb5-4200-ab45-e559b0f20896",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model selection\n",
    "\n",
    "Selecting the model from the epoch with the lowest validation set loss can help prevent overfitting. It requires having a big enough validation set that provides an accurate estimate of performance. \n",
    "\n",
    "We enabled model selection if the validation set size was ≥ 32 for METL-Local and ≥ 128 for METL-Global. We found the optimization was more stable for METL-Local than METL-Global, thus smaller validation sets were still reliable. \n",
    "\n",
    "Enable model selection by setting argument `--ckpt_monitor val`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f773b-8209-4993-b3f0-994b0ab2b133",
   "metadata": {},
   "source": [
    "## Backbone cutoff for METL-Global\n",
    "Finetuning METL-Global is largely the same as METL-Local, except we recommend using a different threshold for model selection (see above), as well as a different backbone cutoff.\n",
    "\n",
    "For METL-Local, we set `--backbone_cutoff -1`, which attaches the new prediction head immediately after the final fully connected layer. \n",
    "\n",
    "For METL-Global, we recommend setting `--backbone_cutoff -2`, which attaches the new prediction head immediately after the global pooling layer. We found this resulted in better finetuning performance for METL-Global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a591eb8-3d5e-437f-9189-3c0834f7f447",
   "metadata": {},
   "source": [
    "# Running inference using finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958aba4-6881-45d9-a566-9d47f81484dc",
   "metadata": {},
   "source": [
    "We provide two ways to run inference: using our PyTorch Lightning-based inference framework or using your own inference loop. The example below shows you would use your own inference loop. See the [inference.ipynb](inference.ipynb) notebook for additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4340777b-fd65-4717-9026-dcdb7f3df581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import inference\n",
    "import encode as enc\n",
    "import utils  # for loading dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55ce3ad-06f0-4da8-9b23-d5738830f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sg/PycharmProjects/metl/code/inference.py:159: UserWarning: Transforming checkpoint keys: strip_prefix='model.', add_prefix=''\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# the Lightning checkpoint from the finetuning we performed above\n",
    "ckpt_fn = \"output/training_logs/inqx2jYi/checkpoints/epoch=49-step=50.ckpt\"\n",
    "model = inference.load_pytorch_module(ckpt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2ccfa9-e012-40e6-8f1b-266275418679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3805],\n",
      "        [-0.3623],\n",
      "        [-0.2217]])\n"
     ]
    }
   ],
   "source": [
    "# load the GFP wild-type sequence and the PDB file (needed for 3D RPE)\n",
    "datasets = utils.load_dataset_metadata()\n",
    "wt = datasets[\"avgfp\"][\"wt_aa\"]\n",
    "wt_offset = datasets[\"avgfp\"][\"wt_ofs\"]\n",
    "pdb_fn = datasets[\"avgfp\"][\"pdb_fn\"]\n",
    "\n",
    "# some example GFP variants to compute the scores for\n",
    "variants = [\"E3K,G102S\",\n",
    "            \"T36P,S203T,K207R\",\n",
    "            \"V10A,D19G,F25S,E113V\"]\n",
    "\n",
    "# encode the variants into the integer sequences format\n",
    "# to feed into the model\n",
    "encoded_variants = enc.encode(\n",
    "    encoding=\"int_seqs\",\n",
    "    variants=variants,\n",
    "    wt_aa=wt,\n",
    "    wt_offset=wt_offset,\n",
    "    indexing=\"0_indexed\"\n",
    ")\n",
    "\n",
    "# set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# no need to compute gradients for inference\n",
    "with torch.no_grad():\n",
    "    # note we are specifying the pdb_fn because this model uses 3D RPE\n",
    "    predictions = model(torch.tensor(encoded_variants), pdb_fn=pdb_fn)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
