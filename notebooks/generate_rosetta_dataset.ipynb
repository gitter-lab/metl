{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adcce7f9-5ca3-46d4-a9f4-19838caed828",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate a Rosetta dataset\n",
    "\n",
    "This notebook shows how to generate a Rosetta dataset, which can be used to pretrain METL models. It assumes you already ran Rosetta simulations using our [metl-sim](https://github.com/gitter-lab/metl-sim) repository. For demonstration purposes, this notebook will use a sample output from that repository ([avgfp_rosettafy_sample.db](../data/rosetta_data/avgfp_rosettafy_sample.db)). \n",
    "\n",
    "1. Create a Rosetta dataset using the script [parse_rosetta_data.py](../code/parse_rosetta_data.py) to parse and format the Rosetta data generated by [metl-sim](https://github.com/gitter-lab/metl-sim). This step is necessary to remove duplicates, handle NaN values, and remove outliers. \n",
    "2. Create train, validation, and test splits for the Rosetta dataset using [split_dataset.py](../code/split_dataset.py).\n",
    "3. Compute standardization parameters from the train set using [compute_rosetta_standardization.py](../code/compute_standardization.py). The standardization parameters are needed during training and evaluation to standardize the various Rosetta energies so that they are on similar scales with mean 0 and standard deviation 1.\n",
    "4. Make sure all PDB files from the Rosetta dataset are present in the [pdb_files](../data/pdb_files) directory and are listed in [pdb_index.csv](../data/rosetta_data/pdb_index.csv).\n",
    "\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "The Rosetta dataset generated by running this notebook is already saved in this repository in the [data/rosetta_data/avgfp](../data/rosetta_data/avgfp) directory. If you would like to run this notebook, first delete or rename that directory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d048b60dd2c1635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T22:51:36.573559Z",
     "start_time": "2024-02-16T22:51:36.569490Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb94936-8646-4154-8e7c-2adb7e1a3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# define the name of the project root directory\n",
    "project_root_dir_name = \"metl\"\n",
    "\n",
    "# find the project root by checking each parent directory\n",
    "current_dir = os.getcwd()\n",
    "while os.path.basename(current_dir) != project_root_dir_name and current_dir != os.path.dirname(current_dir):\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# change the current working directory to the project root directory\n",
    "if os.path.basename(current_dir) == project_root_dir_name:\n",
    "    os.chdir(current_dir)\n",
    "else:\n",
    "    print(\"project root directory not found\")\n",
    "    \n",
    "# add the project code folder to the system path so imports work\n",
    "module_path = os.path.abspath(\"code\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76a245-66d4-4ea0-8032-bb55b44ba740",
   "metadata": {},
   "source": [
    "# Parse Rosetta data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41dc96-a33f-4801-a88f-53566fbb0a85",
   "metadata": {},
   "source": [
    "Parsing rosetta data is handled by the [parse_rosetta_data.py](../code/parse_rosetta_data.py) script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8501490a-5bdd-44e7-b5e5-fc659f0551f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: parse_rosetta_data.py [-h] [--ds_name DS_NAME] [--pdb_fns PDB_FNS]\n",
      "                             [--db_fn DB_FN]\n",
      "                             [--keep_num_muts KEEP_NUM_MUTS [KEEP_NUM_MUTS ...]]\n",
      "                             [--remove_nan] [--no_remove_nan]\n",
      "                             [--remove_duplicates] [--no_remove_duplicates]\n",
      "                             [--remove_outliers] [--no_remove_outliers]\n",
      "                             [--outlier_energy_term OUTLIER_ENERGY_TERM]\n",
      "                             [--outlier_threshold OUTLIER_THRESHOLD]\n",
      "                             [--replace_pdb_fn REPLACE_PDB_FN] [--ct_fn CT_FN]\n",
      "                             {generate_dataset,generate_dms_coverage_dataset}\n",
      "\n",
      "positional arguments:\n",
      "  {generate_dataset,generate_dms_coverage_dataset}\n",
      "                        mode to run\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --ds_name DS_NAME     name of the dataset to generate\n",
      "  --pdb_fns PDB_FNS     either the path to a single pdb file or path to a file\n",
      "                        containing a list of pdb filenames to include in the\n",
      "                        dataset\n",
      "  --db_fn DB_FN         path to the variant database created in rosettafy\n",
      "  --keep_num_muts KEEP_NUM_MUTS [KEEP_NUM_MUTS ...]\n",
      "                        list of number of mutations to keep (for example, to\n",
      "                        create a singles-only dataset)\n",
      "  --remove_nan          remove any variants with NaN values (default)\n",
      "  --no_remove_nan\n",
      "  --remove_duplicates   remove duplicate variants (default)\n",
      "  --no_remove_duplicates\n",
      "  --remove_outliers     remove outliers using median absolute deviation method\n",
      "                        (default)\n",
      "  --no_remove_outliers\n",
      "  --outlier_energy_term OUTLIER_ENERGY_TERM\n",
      "                        the energy term to use for outlier removal\n",
      "  --outlier_threshold OUTLIER_THRESHOLD\n",
      "                        the threshold for outlier removal\n",
      "  --replace_pdb_fn REPLACE_PDB_FN\n",
      "                        replace the PDB filename in the dataset with this\n",
      "                        filename\n",
      "  --ct_fn CT_FN         path to the SQL create tables file\n"
     ]
    }
   ],
   "source": [
    "%run code/parse_rosetta_data.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891ca81-1984-413d-9e5a-39b1023fa246",
   "metadata": {},
   "source": [
    "## Define arguments and run this script on our sample database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6356db16-f0fb-4843-b65d-fe9418f6c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset name for our Rosetta dataset\n",
    "# call this one \"avgfp\" because it's for pretraining a METL-Local avGFP model\n",
    "ds_name = \"avgfp\"\n",
    "\n",
    "# path to the metl-sim database containing the raw Rosetta data\n",
    "# using the sample database provided in this repository\n",
    "db_fn = \"data/rosetta_data/avgfp_rosettafy_sample.db\"\n",
    "\n",
    "# what pdb files to include in this Rosetta dataset\n",
    "# this is necessary if, for example, your raw metl-sim database contains multiple different \n",
    "# PDB files, but you only want to create a Rosetta dataset for one of them\n",
    "# the avGFP PDB file is \"1gfl_cm.pdb\", so that's what we'll use\n",
    "# note, if you want to use multiple PDB files (like for a METL-Global rosetta dataset), \n",
    "# you can specify a path to a text file containing a list of PDBs to include\n",
    "pdb_fns = \"1gfl_cm.pdb\"\n",
    "\n",
    "# removing outliers\n",
    "outlier_energy_term = \"total_score\"\n",
    "outlier_threshold = 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f9d3d0-a08b-4a5a-9007-aab737a5827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:METL.__main__:output data directory will be: data/rosetta_data/avgfp\n",
      "INFO:METL.__main__:connecting to database at: data/rosetta_data/avgfp_rosettafy_sample.db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_count: 10000\n",
      "process_list: []\n",
      "process_list_ds: []\n",
      "initial data loaded into dataframe\n",
      "Filtering variants with NaN values\n",
      "Dropped 0 variants with nan values\n",
      "Num variants after NaN filter: 10000\n",
      "Removed 1 duplicates\n",
      "Num variants after duplicate filter: 9999\n",
      "Removed 87 outliers\n",
      "Num variants after outlier removal: 9912\n",
      "Saving dataset to CSV\n",
      "Saving dataset to HDF, pandas fixed format\n",
      "Saving dataset to SQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 9912/9912 [00:00<00:00, 122878.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# run parse_rosetta_data.py with the arguments defined above\n",
    "# additionally, we will set the flags:\n",
    "# --remove_nan (removes variants with NaN values for any of the energy terms)\n",
    "# --remove_duplicates (removes duplicate variants)\n",
    "# --remove_outliers (removes outliers)\n",
    "%run code/parse_rosetta_data.py generate_dataset --ds_name $ds_name --db_fn $db_fn --pdb_fns $pdb_fns --remove_nan --remove_duplicates --remove_outliers --outlier_energy_term $outlier_energy_term --outlier_threshold $outlier_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e36c7b-97ce-4389-bae8-b459e729b39e",
   "metadata": {},
   "source": [
    "# Create train, validation, and test splits\n",
    "\n",
    "Functions for creating training splits are contained in the [split_dataset.py](../code/split_dataset.py) file.\n",
    "This file does not have a command line interface, so you will call functions directly after importing it into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd42a7f7-70c8-4786-bcd6-286a4f1654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import split_dataset as sd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c52488-1359-409e-887f-d128b6767ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the .h5 dataset created above\n",
    "ds_fn = \"data/rosetta_data/avgfp/avgfp.h5\"\n",
    "\n",
    "# define an output directory for placing the splits\n",
    "out_dir = \"data/rosetta_data/avgfp/splits\"\n",
    "\n",
    "# load the dataset\n",
    "ds = pd.read_hdf(ds_fn, key=\"variant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323fe69-cb4a-460c-b764-b30acd4b1456",
   "metadata": {},
   "source": [
    "We will first withhold 5% of the data to be used as a \"super test\" set using the `supertest()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650c1545-0c0b-4fd2-b210-ba25a1a37073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:METL.split_dataset:saving supertest split to file data/rosetta_data/avgfp/splits/supertest_w1aea30517f4f_s0.05_r7808.txt\n"
     ]
    }
   ],
   "source": [
    "# for purposes of this demonstration, make random seed constant\n",
    "# rseed1 = random.randint(1000, 9999)\n",
    "rseed1 = 7808\n",
    "_, withhold_fn = sd.supertest(ds, size=0.05, rseed=rseed1, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee22fb-99e9-43af-8306-6772c389a0b0",
   "metadata": {},
   "source": [
    "Now we create standard 80% train, 10% validation, and 10% test sets using the `train_val_test()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f0d597-9760-4f9c-aee0-cb944d8e8749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:METL.split_dataset:saving train-val-test split to directory data/rosetta_data/avgfp/splits/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991\n"
     ]
    }
   ],
   "source": [
    "# for purposes of this demonstration, make random seed constant\n",
    "# rseed2 = random.randint(1000, 9999)\n",
    "rseed2 = 4991\n",
    "split, out_dir_split = sd.train_val_test(ds, train_size=0.8, val_size=0.1, test_size=0.1, withhold=withhold_fn, rseed=rseed2, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1dd302-d52c-4b1b-bb04-9548dba3a2ed",
   "metadata": {},
   "source": [
    "# Compute standardization parameters\n",
    "The Rosetta energy terms are on different scales, and to make training easier we standardize them to have mean 0 and standard deviation 1. The standardization parameters are computed on the train set only. The script [compute_rosetta_standardization.py](../code/compute_standardization.py) will compute the standardization parameters and store them in the given split directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420019e0-f1f7-4ea9-b7b5-3c45f3e2582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: compute_rosetta_standardization.py [-h] [--split_dir SPLIT_DIR]\n",
      "                                          [--energies_start_col ENERGIES_START_COL]\n",
      "                                          ds_fn_h5\n",
      "\n",
      "positional arguments:\n",
      "  ds_fn_h5              path to the rosetta dataset in hdf5 format\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --split_dir SPLIT_DIR\n",
      "                        path to the split directory containing the\n",
      "                        train/val/test split indices. if provided, the\n",
      "                        standardization parameters will be computed on the\n",
      "                        training set only. this is necessary for training a\n",
      "                        source model.\n",
      "  --energies_start_col ENERGIES_START_COL\n",
      "                        the column name of the first energy term in the\n",
      "                        dataset. default is 'total_score'. this is used to\n",
      "                        determine which columns in the dataset are energy\n",
      "                        terms. leave this as default unless for some reason\n",
      "                        total_score is not the first energy term.\n"
     ]
    }
   ],
   "source": [
    "%run code/compute_rosetta_standardization.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d33222cf-d0d5-4c73-8360-21d647f9ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:METL.__main__:computing standardization params on training set only\n",
      "INFO:METL.__main__:saving standardization params to: data/rosetta_data/avgfp/splits/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991/standardization_params\n"
     ]
    }
   ],
   "source": [
    "# compute the standardization parameters for the dataset and train split we created above\n",
    "ds_fn_h5 = \"data/rosetta_data/avgfp/avgfp.h5\"\n",
    "split_dir = \"data/rosetta_data/avgfp/splits/standard_tr0.8_tu0.1_te0.1_w1aea30517f4f_r4991\"\n",
    "%run code/compute_rosetta_standardization.py $ds_fn_h5 --split_dir $split_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff0ea8-ca91-416a-9436-3fc994a01b82",
   "metadata": {},
   "source": [
    "# Ensure PDB files are listed in the index\n",
    "PDB files in the Rosetta dataset need to be present in the [pdb_files](../data/pdb_files) directory and listed in [pdb_index.csv](../data/rosetta_data/pdb_index.csv) file. This is necessary because the 3D relative position embedding needs access to the PDB files and some of the training code will reference the index file. \n",
    "\n",
    "This bit of code will automatically generate the pdb_index.csv based on the pdb files in the pdb_files directory. Run this any time you add pdb files to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3037a70-ab0d-4ee4-88d5-239e1bc977e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/pdb_files/1gfl_cm.pdb\n"
     ]
    }
   ],
   "source": [
    "pdb_dir = \"data/pdb_files\"\n",
    "pdb_fns = [join(pdb_dir, x) for x in os.listdir(pdb_dir) if x.endswith(\".pdb\")]\n",
    "with open(\"data/rosetta_data/pdb_index.csv\", \"w\") as f:\n",
    "    f.write(\"pdb_fn,aa_sequence,seq_len\\n\")\n",
    "    for pdb_fn in pdb_fns:\n",
    "        print(\"Processing {}\".format(pdb_fn))\n",
    "        seq = utils.extract_seq_from_pdb(pdb_fn)\n",
    "        f.write(\"{},{},{}\\n\".format(basename(pdb_fn), seq, len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3727d5-a6a3-47d6-a112-6ad969d48597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
